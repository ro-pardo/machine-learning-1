{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:ab4d4ffcbe249b6cff3a4075a55ec9c874bec51678eeb4e5d94fe64ebd114c06"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy,sklearn,sklearn.datasets,utils\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Principal Component Analysis\n",
      "\n",
      "In this exercise, we will experiment with two different techniques to compute the PCA components of a dataset:\n",
      "\n",
      "* **Standard PCA**: The standard technique based on eigenvalue decomposition.\n",
      "\n",
      "* **Iterative PCA**: A technique that iteratively optimizes the PCA objective.\n",
      "\n",
      "We consider a random subset of the Labeled Faces in the Wild (LFW) dataset, readily accessible from sklearn, and we apply some basic preprocessing to discount strong variations of luminosity and contrast."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = sklearn.datasets.fetch_lfw_people(resize=0.5)['images']\n",
      "X = X[numpy.random.mtrand.RandomState(1).permutation(len(X))[:150]]*1.0\n",
      "X = X - X.mean(axis=(1,2),keepdims=True)\n",
      "X = X / X.std(axis=(1,2),keepdims=True)\n",
      "print(X.shape)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two functions are provided for your convenience and are available in `utils.py` that is included in the zip archive. The functions are the following:\n",
      "\n",
      "* **`utils.scatterplot`** produces a scatter plot from a two-dimensional data set.\n",
      "\n",
      "* **`utils.render`** takes an array of data points or objects of similar shape, and renders them in the IPython notebook.\n",
      "\n",
      "Some demo code that makes use of these functions is given below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "utils.scatterplot(X[:,32,20],X[:,32,21]) # Plot relation between adjacent pixels\n",
      "utils.render(X[:30],15,2,vmax=5)         # Display first 10 examples in the data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PCA with Eigenvalue Decomposition (15 P)\n",
      "\n",
      "Principal components can be found by solving the eigenvalue problem\n",
      "\n",
      "$$\n",
      "S \\boldsymbol{w} = \\lambda \\boldsymbol{w}.\n",
      "$$\n",
      "\n",
      "where  $S = \\sum_{k=1}^N (\\boldsymbol{x}_k - \\boldsymbol{m}) (\\boldsymbol{x}_k - \\boldsymbol{m})^\\top$ is the scatter matrix, and where $\\boldsymbol{m} = \\frac1N \\sum_{k=1}^N \\boldsymbol{x}_k$ is the mean vector.\n",
      "\n",
      "**Tasks:**\n",
      "\n",
      "* **Compute the principal components of the data using the function `numpy.linalg.eigh`.**\n",
      "* **Measure the computational time required to find the principal components. Use the function `time.time()` for that purpose. Do *not* include in your estimate the computation overhead caused by loading the data, plotting and rendering.**\n",
      "* **Plot the projection of the dataset on the first two principal components using the function `utils.scatterplot`.**\n",
      "* **Visualize the 60 leading principal components using the function `utils.render`.**\n",
      "\n",
      "Note that if the algorithm runs for more than 1 minute, there may be some error in your implementation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### REPLACE BY YOUR CODE\n",
      "import solutions; solutions.basic(X)\n",
      "###"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When looking at the scatter plot, we observe that much more variance is expressed in the first two principal components than in individual dimensions as it was plotted before. When looking at the principal components themselves which we render as images, we can see that the first principal components correspond to low-frequency filters that select for coarse features, and the following principal components capture progressively higher-frequency information and are also becoming more noisy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Iterative PCA (15 P)\n",
      "\n",
      "The standard PCA method based on eigenvalues is quite expensive to compute. Instead, the power iteration algorithm looks only for the first component and finds it using an iterative procedure. It starts with an initial weight vector $\\boldsymbol{w}$, and repeatedly applies the update rule\n",
      "\n",
      "$$\n",
      "\\boldsymbol{w} \\leftarrow S \\boldsymbol{w} \\,\\big/\\, \\|S \\boldsymbol{w}\\|.\n",
      "$$\n",
      "\n",
      "Like for standard PCA, the objective that iterative PCA optimizes is $J(\\boldsymbol{w}) = \\boldsymbol{w}^\\top S \\boldsymbol{w}$ subject to the unit norm constraint for $\\boldsymbol{w}$. We can therefore keep track of the progress of the algorithm after each iteration.\n",
      "\n",
      "**Tasks:**\n",
      "\n",
      "* **Implement the iterative PCA algorithm. Use as a stopping criterion the value of $J(\\boldsymbol{w})$ between two iterations increasing by less than one.**\n",
      "* **Print the value of the objective function $J(\\boldsymbol{w})$ at each iteration.**\n",
      "* **Measure the time taken to find the principal component.**\n",
      "* **Visualize the the eigenvector $\\boldsymbol{w}$ obtained after convergence using the function `utils.render`.**\n",
      "\n",
      "Note that if the algorithm runs for more than 1 minute, there may be some error in your implementation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### REPLACE BY YOUR CODE\n",
      "import solutions; solutions.iterative(X)\n",
      "###"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We observe that the computation time has decreased significantly. The difference of performance becomes larger as the number of dimensions increases. We can observe that the principal component is the same (sometimes up to a sign flip) as the one obtained by standard PCA."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}